{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0e5f51",
   "metadata": {},
   "source": [
    "# Writing Web Scrapping Code to Collect Images From Google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9ebb7",
   "metadata": {},
   "source": [
    "## loading the Libraries Necessaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32dc902",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sriram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14d5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os \n",
    "import requests\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be428f40",
   "metadata": {},
   "source": [
    "## Which Person Image do you Want to download And How much Images Do you Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a5a03e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"My favourite\"\n",
    "actress = \"billi ellish\"\n",
    "Images_needed = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d882c",
   "metadata": {},
   "source": [
    "## Setting according to your prefrences (with and without browser opening while collecting image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b8901e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To run without browser opening\n",
    "# op = webdriver.ChromeOptions()\n",
    "# op.add_argument('headless')\n",
    "# driver = webdriver.Chrome(options=op)\n",
    "# driver.get(\"https://google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f74fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to open browser while running (we can visualize)\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1447d6c9",
   "metadata": {},
   "source": [
    "## Setting the SearchBar in google And Entering the className"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d49342ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = driver.find_element(\"xpath\",\"/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/input\")\n",
    "box.send_keys(actress)\n",
    "box.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd0fe3b",
   "metadata": {},
   "source": [
    "## moving to the images tab in google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "810a6db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(\"xpath\",\"//*[@id='hdtb-msb']/div[1]/div/div[2]/a\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbc468",
   "metadata": {},
   "source": [
    "## Getting the right amount of images needed from all the images in google images tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29139ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished_scrolling\n"
     ]
    }
   ],
   "source": [
    "starting_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "finished_length = starting_length+1\n",
    "Find_Images = True\n",
    "while (starting_length != finished_length) & (Find_Images):\n",
    "    starting_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
    "    all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "    try:\n",
    "        all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "        if len(all_images) <= Images_needed:\n",
    "            time.sleep(5)\n",
    "            finished_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        else:\n",
    "            all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "            Find_Images = False\n",
    "    except Exception as e:\n",
    "        print(f\"the exception occured as: {e}\")\n",
    "print(\"finished_scrolling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6913023a",
   "metadata": {},
   "source": [
    "## Checking how many images we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e46386dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d36ce9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "len(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13b040",
   "metadata": {},
   "source": [
    "## Creating a folder with a Class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "78be81f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(),f\"{class_name}\")\n",
    "os.makedirs(path,exist_ok=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "67296e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516fcf21",
   "metadata": {},
   "source": [
    "## Final Total Number Of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "214f3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #to get a image file without compression\n",
    "# import urllib.request\n",
    "# image_url = \"give any image url from google we can get a image file without compression :)\"\n",
    "# request = urllib.request.Request(image_url, headers={\"Accept-Encoding\": \"identity\"})\n",
    "# response = urllib.request.urlopen(request)\n",
    "# with open(\"image.jpg\", \"wb\") as f:\n",
    "#     f.write(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2a50322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,image_link in enumerate(all_images):\n",
    "    image_url = image_link.find(\"img\",class_=\"rg_i Q4LuWd\").get(\"src\")\n",
    "    alt_image_url = image_link.find(\"img\",class_=\"rg_i Q4LuWd\").get(\"data-src\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(image_url,f\"{path}/{actress}_image_{i}.jpg\")\n",
    "    except Exception as e:\n",
    "        urllib.request.urlretrieve(alt_image_url,f\"{path}/{actress}image_{i}.jpg\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3250eb82",
   "metadata": {},
   "source": [
    "## total Number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "66940fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facf737a",
   "metadata": {},
   "source": [
    "# Doing it all at once "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12a5a1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e04f1d6",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current actress/actor is ->> millie bobby brown <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 20 images\n",
      "the current actress/actor is ->> will smith <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 0 images\n",
      "the current actress/actor is ->> CARA DELEVINGNE <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 20 images\n",
      "the current actress/actor is ->> FELICITY JONES <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 20 images\n",
      "the current actress/actor is ->> Avatar <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 0 images\n",
      "the current actress/actor is ->> olivia rodrigo <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 20 images\n",
      "the current actress/actor is ->> brie larson <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 20 images\n",
      "the current actress/actor is ->> priyanka chopra <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 20 images\n",
      "the current actress/actor is ->> Alia Bhatt <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 20 images\n",
      "the current actress/actor is ->> deepika padukone <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 20 images\n",
      "the current actress/actor is ->> Ebony maw <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 20 images\n",
      "the current actress/actor is ->> russo brothers <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 0 images\n",
      "++++++++++++++++++++\n",
      "Every Image in the Actress Variable is Downloaded\n",
      "And the total images we have in our = My not so favourite -.- class is ->> 180\n"
     ]
    }
   ],
   "source": [
    "class_name = \"My not so favourite\"\n",
    "actress = [\"millie bobby brown\",\"will smith\",\"CARA DELEVINGNE\",\"FELICITY JONES\",\"Avatar\",\"olivia rodrigo\",\"brie larson\",\n",
    "          \"priyanka chopra\",\"Alia Bhatt\",\"deepika padukone\",\"Ebony maw\",\"russo brothers\"]\n",
    "# class_name = \"My favourite\"\n",
    "data = \"Train\"\n",
    "# actress = [\"billie ellish\",\"conan o brien\",\"deadpool\",\"handsome squid\",\"jim carry\",\"mila kunis\",\"pepe the frog\",\"Phobe buffay\",\"samantha\",\"Trisha\",\"Zendya\"]\n",
    "Images_needed = 20\n",
    "\n",
    "for actress in actress:\n",
    "    print(f\"the current actress/actor is ->> {actress} <<-\")\n",
    "    #to run with browser\n",
    "#     driver = webdriver.Chrome()\n",
    "#     driver.get(\"https://google.com\")\n",
    "    #or\n",
    "    #To run without browser opening\n",
    "    op = webdriver.ChromeOptions()\n",
    "    op.add_argument('headless')\n",
    "    driver = webdriver.Chrome(options=op)\n",
    "    driver.get(\"https://google.com\")\n",
    "    box = driver.find_element(\"xpath\",\"/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/input\")\n",
    "    box.send_keys(actress)\n",
    "    box.send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "    driver.find_element(\"xpath\",\"//*[@id='hdtb-msb']/div[1]/div/div[2]/a\").click()\n",
    "\n",
    "    starting_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    finished_length = starting_length+1\n",
    "    Find_Images = True\n",
    "    while (starting_length != finished_length) & (Find_Images):\n",
    "        starting_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "        soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
    "        all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "        try:\n",
    "            all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "            if len(all_images) <= Images_needed:\n",
    "                time.sleep(5)\n",
    "                finished_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            else:\n",
    "                all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "                Find_Images = False\n",
    "        except Exception as e:\n",
    "            print(f\"the exception occured as: {e}\")\n",
    "    print(\"finished_scrolling\")\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "    all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "\n",
    "\n",
    "    path = os.path.join(os.getcwd(),f\"{data}\",f\"{class_name}\")\n",
    "    os.makedirs(path,exist_ok=True)  \n",
    "\n",
    "    for i,image_link in enumerate(all_images[:Images_needed]):\n",
    "        image_url = image_link.find(\"img\",class_=\"rg_i Q4LuWd\").get(\"src\")\n",
    "        alt_image_url = image_link.find(\"img\",class_=\"rg_i Q4LuWd\").get(\"data-src\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(image_url,f\"{path}/{actress}_image_{i}.jpg\")\n",
    "        except Exception as e:\n",
    "            urllib.request.urlretrieve(alt_image_url,f\"{path}/{actress}image_{i}.jpg\")\n",
    "            pass\n",
    "    print(f\"Downloaded Complete and we got :-> {len(all_images[:Images_needed])} images\")\n",
    "print(\"+\"*20)\n",
    "print(\"Every Image in the Actress Variable is Downloaded\")\n",
    "print(f\"And the total images we have in our = {class_name} -.- class is ->> {len(os.listdir(path))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a3b8a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Validation Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69afb073",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current actress/actor is ->> billie ellish <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 70 images\n",
      "the current actress/actor is ->> conan o brien <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 0 images\n",
      "the current actress/actor is ->> deadpool <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 70 images\n",
      "the current actress/actor is ->> handsome squid <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 70 images\n",
      "the current actress/actor is ->> jim carry <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 70 images\n",
      "the current actress/actor is ->> mila kunis <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 70 images\n",
      "the current actress/actor is ->> pepe the frog <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 70 images\n",
      "the current actress/actor is ->> Phobe buffay <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 70 images\n",
      "the current actress/actor is ->> samantha <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 70 images\n",
      "the current actress/actor is ->> Trisha <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 70 images\n",
      "the current actress/actor is ->> Zendya <<-\n",
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 70 images\n",
      "++++++++++++++++++++\n",
      "Every Image in the Actress Variable is Downloaded\n",
      "And the total images we have in our = My favourite -.- class is ->> 40\n"
     ]
    }
   ],
   "source": [
    "# class_name = \"My not so favourite\"\n",
    "# actress = [\"millie bobby brown\",\"will smith\",\"CARA DELEVINGNE\",\"FELICITY JONES\",\"Avatar\",\"olivia rodrigo\",\"brie larson\",\n",
    "#           \"priyanka chopra\",\"Alia Bhatt\",\"deepika padukone\",\"Ebony maw\",\"russo brothers\"]\n",
    "class_name = \"My favourite\"\n",
    "data = \"Valid\"\n",
    "actress = [\"billie ellish\",\"conan o brien\",\"deadpool\",\"handsome squid\",\"jim carry\",\"mila kunis\",\"pepe the frog\",\"Phobe buffay\",\"samantha\",\"Trisha\",\"Zendya\"]\n",
    "Images_needed = 30\n",
    "Images_starting = int(-20*0.2)\n",
    "\n",
    "for actress in actress:\n",
    "    print(f\"the current actress/actor is ->> {actress} <<-\")\n",
    "    #to run with browser\n",
    "#     driver = webdriver.Chrome()\n",
    "#     driver.get(\"https://google.com\")\n",
    "    #or\n",
    "    #To run without browser opening\n",
    "    op = webdriver.ChromeOptions()\n",
    "    op.add_argument('headless')\n",
    "    driver = webdriver.Chrome(options=op)\n",
    "    driver.get(\"https://google.com\")\n",
    "    box = driver.find_element(\"xpath\",\"/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/input\")\n",
    "    box.send_keys(actress)\n",
    "    box.send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "    driver.find_element(\"xpath\",\"//*[@id='hdtb-msb']/div[1]/div/div[2]/a\").click()\n",
    "\n",
    "    starting_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    finished_length = starting_length+1\n",
    "    Find_Images = True\n",
    "    while (starting_length != finished_length) & (Find_Images):\n",
    "        starting_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "        soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
    "        all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "        try:\n",
    "            all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "            if len(all_images) <= Images_needed:\n",
    "                time.sleep(5)\n",
    "                finished_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            else:\n",
    "                all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "                Find_Images = False\n",
    "        except Exception as e:\n",
    "            print(f\"the exception occured as: {e}\")\n",
    "    print(\"finished_scrolling\")\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "    all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "\n",
    "\n",
    "    path = os.path.join(os.getcwd(),f\"{data}\",f\"{class_name}\")\n",
    "    os.makedirs(path,exist_ok=True)  \n",
    "\n",
    "    for i,image_link in enumerate(all_images[Images_starting:]):\n",
    "        image_url = image_link.find(\"img\",class_=\"rg_i Q4LuWd\").get(\"src\")\n",
    "        alt_image_url = image_link.find(\"img\",class_=\"rg_i Q4LuWd\").get(\"data-src\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(image_url,f\"{path}/{actress}_image_{i}.jpg\")\n",
    "        except Exception as e:\n",
    "            urllib.request.urlretrieve(alt_image_url,f\"{path}/{actress}image_{i}.jpg\")\n",
    "            pass\n",
    "    print(f\"Downloaded Complete and we got :-> {len(all_images[Images_needed:])} images\")\n",
    "print(\"+\"*20)\n",
    "print(\"Every Image in the Actress Variable is Downloaded\")\n",
    "print(f\"And the total images we have in our = {class_name} -.- class is ->> {len(os.listdir(path))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e6f4f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train Single Image Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa33dcd0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished_scrolling\n",
      "Downloaded Complete and we got :-> 30 images\n",
      "++++++++++++++++++++\n",
      "Every Image in the Actress Variable is Downloaded\n",
      "And the total images we have in our = My not so favourite -.- class is ->> 240\n"
     ]
    }
   ],
   "source": [
    "actress = \"ruso brothers\"\n",
    "data = \"Train\"\n",
    "class_name = \"My not so favourite\"\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.google.com/search?q=will+smith&rlz=1C1CHBF_enIN1002IN1002&sxsrf=ALiCzsb0760GAbiA7Sis_t1GbLrmQ7I8xQ:1672301269347&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiBpvfYr578AhW2SWwGHZsSDv0Q_AUoAnoECAEQBA&biw=1366&bih=657&dpr=1\"\n",
    "driver.get(url)\n",
    "starting_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "finished_length = starting_length+1\n",
    "Find_Images = True\n",
    "while (starting_length != finished_length) & (Find_Images):\n",
    "    starting_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
    "    all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "    try:\n",
    "        all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "        if len(all_images) <= Images_needed:\n",
    "            time.sleep(5)\n",
    "            finished_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        else:\n",
    "            all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "            Find_Images = False\n",
    "    except Exception as e:\n",
    "        print(f\"the exception occured as: {e}\")\n",
    "print(\"finished_scrolling\")\n",
    "soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "\n",
    "\n",
    "path = os.path.join(os.getcwd(),f\"{data}\",f\"{class_name}\")\n",
    "os.makedirs(path,exist_ok=True)  \n",
    "\n",
    "for i,image_link in enumerate(all_images[:Images_needed]):\n",
    "    image_url = image_link.find(\"img\",class_=\"rg_i Q4LuWd\").get(\"src\")\n",
    "    alt_image_url = image_link.find(\"img\",class_=\"rg_i Q4LuWd\").get(\"data-src\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(image_url,f\"{path}/{actress}_image_{i}.jpg\")\n",
    "    except Exception as e:\n",
    "        urllib.request.urlretrieve(alt_image_url,f\"{path}/{actress}image_{i}.jpg\")\n",
    "        pass\n",
    "print(f\"Downloaded Complete and we got :-> {len(all_images[:Images_needed])} images\")\n",
    "print(\"+\"*20)\n",
    "print(\"Every Image in the Actress Variable is Downloaded\")\n",
    "print(f\"And the total images we have in our = {class_name} -.- class is ->> {len(os.listdir(path))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911c4ac",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Valid Single Image Donwload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad22b07f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished_scrolling\n",
      "48\n",
      "Downloaded Complete and we got :-> 18 images\n",
      "++++++++++++++++++++\n",
      "Every Image in the Actress Variable is Downloaded\n",
      "And the total images we have in our = My favourite -.- class is ->> 44\n"
     ]
    }
   ],
   "source": [
    "actress = \"conan o brien\"\n",
    "class_name = \"My favourite\"\n",
    "data = \"Valid\"\n",
    "Images_needed = 30\n",
    "Images_starting = int(-20*0.2)\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.google.com/search?q=conan+o+brien&rlz=1C1CHBF_enIN1002IN1002&sxsrf=ALiCzsbPTcoX45sagPygfUrd5nYfYzOE3Q:1672302100892&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjn47jlsp78AhU6T2wGHfBYDeoQ_AUoAnoECAEQBA&biw=1366&bih=657&dpr=1\"\n",
    "driver.get(url)\n",
    "\n",
    "starting_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "finished_length = starting_length+1\n",
    "Find_Images = True\n",
    "while (starting_length != finished_length) & (Find_Images):\n",
    "    starting_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
    "    all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "    try:\n",
    "        all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "        if len(all_images) <= Images_needed:\n",
    "            time.sleep(5)\n",
    "            finished_length = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        else:\n",
    "            all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "            Find_Images = False\n",
    "    except Exception as e:\n",
    "        print(f\"the exception occured as: {e}\")\n",
    "print(\"finished_scrolling\")\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "all_images = soup.find_all(\"div\",class_=\"isv-r PNCib MSM1fd BUooTd\")\n",
    "print(len(all_images))\n",
    "\n",
    "path = os.path.join(os.getcwd(),f\"{data}\",f\"{class_name}\")\n",
    "os.makedirs(path,exist_ok=True)  \n",
    "\n",
    "for i,image_link in enumerate(all_images[Images_starting:]):\n",
    "    image_url = image_link.find(\"img\",class_=\"rg_i Q4LuWd\").get(\"src\")\n",
    "    alt_image_url = image_link.find(\"img\",class_=\"rg_i Q4LuWd\").get(\"data-src\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(image_url,f\"{path}/{actress}_image_{i}.jpg\")\n",
    "    except Exception as e:\n",
    "        urllib.request.urlretrieve(alt_image_url,f\"{path}/{actress}image_{i}.jpg\")\n",
    "        pass\n",
    "print(f\"Downloaded Complete and we got :-> {len(all_images[Images_needed:])} images\")\n",
    "print(\"+\"*20)\n",
    "print(\"Every Image in the Actress Variable is Downloaded\")\n",
    "print(f\"And the total images we have in our = {class_name} -.- class is ->> {len(os.listdir(path))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea26c14d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "db29d9c3531fbefb8528bf1617aed628388b45e6ce014ed5e5e3db8968ae6306"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
